{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import models, transforms\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def downsamplingBlock(in_features, out_features):\n    block = nn.Sequential(nn.Conv2d(in_features, out_features, 3, padding = 1),\n                         nn.ReLU(),\n                         nn.BatchNorm2d(out_features),\n                         nn.Conv2d(out_features, out_features, 3, padding = 1),\n                         nn.ReLU(),\n                         nn.BatchNorm2d(out_features))\n    return block\ndef upsamplingBlock(in_features, mid_features, out_features):\n    block = nn.Sequential(nn.Conv2d(in_features, mid_features, 3, padding = 1), \n                         nn.ReLU(), \n                         nn.BatchNorm2d(mid_features),\n                         nn.Conv2d(mid_features, mid_features, 3, padding = 1), \n                         nn.ReLU(),\n                         nn.BatchNorm2d(mid_features),\n                         nn.ConvTranspose2d(mid_features, out_features, 3, padding = 1, stride = 2, output_padding = 1))\n    return block\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n        self.pool = nn.MaxPool2d(2)\n        self.block1 = downsamplingBlock(3, 32)\n        self.block2 = downsamplingBlock(32, 64)\n        self.block3 = downsamplingBlock(64, 128)\n        self.bottom = upsamplingBlock(128, 256, 128)\n        self.block4 = upsamplingBlock(256, 128, 64)\n        self.block5 = upsamplingBlock(128, 64, 32)\n        self.final = nn.Sequential(nn.Conv2d(64, 32, 3, padding = 1),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(32),\n                                   nn.Conv2d(32, 32, 3, padding = 1),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(32),\n                                   nn.Conv2d(32, 3, 3, padding = 1))\n    def forward(self, x):\n        out1 = self.block1(x)\n        out2 = self.pool(out1)\n        out2 = self.block2(out2)\n        out3 = self.pool(out2)\n        out3 = self.block3(out3)\n        bottom1 = self.pool(out3)\n        bottom1 = self.bottom(bottom1)\n        concat1 = torch.cat((out3, bottom1), 1)\n        concat1 = self.block4(concat1)\n        concat2 = torch.cat((out2, concat1), 1)\n        concat2 = self.block5(concat2)\n        concat3 = torch.cat((out1, concat2), 1)\n        output = self.final(concat3)\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Algorithms are taken from \"A Neural Algorithm of Artistic Style\", Gatys et al., https://arxiv.org/abs/1508.06576"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PretrainedContentNetwork(nn.Module):\n    def __init__(self): \n        super(PretrainedContentNetwork, self).__init__()\n        self.sota = models.vgg19(pretrained = True)\n        for el in self.sota.parameters():\n            el.requires_grad = False\n        for i in ([4, 9, 18, 27, 36]):\n            self.sota.features[i] = nn.AvgPool2d(2, 2)\n    def forward(self, x):\n        for i in range(12):\n            x = self.sota.features[i](x)\n        x = x.view(-1)\n        return x\n    \nclass PretrainedStyleNetwork(nn.Module):\n    def __init__(self): \n        super(PretrainedStyleNetwork, self).__init__()\n        self.sota = models.vgg19(pretrained = True)\n        for el in self.sota.parameters():\n            el.requires_grad = False\n        for i in ([4, 9, 18, 27, 36]):\n            self.sota.features[i] = nn.AvgPool2d(2, 2)\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for i in range(2):\n            self.slice1.add_module(str(i), self.sota.features[i])\n        for i in range(2, 7):\n            self.slice2.add_module(str(i), self.sota.features[i])\n        for i in range(7, 12):\n            self.slice3.add_module(str(i), self.sota.features[i])\n        for i in range(12, 21):\n            self.slice4.add_module(str(i), self.sota.features[i])\n        for i in range(21, 30):\n            self.slice5.add_module(str(i), self.sota.features[i])\n        del(self.sota)\n    def forward(self, x):\n        self.batch_size = x.size()[0]\n        x = self.slice1(x)\n        features1 = x.view(self.batch_size, x.size()[1], -1)\n        gram1 = torch.matmul(features1, torch.transpose(features1, 1, 2)) / features1.size()[1]\n        x = self.slice2(x)\n        features2 = x.view(self.batch_size, x.size()[1], -1)\n        gram2 = torch.matmul(features2, torch.transpose(features2, 1, 2)) / features2.size()[1]\n        x = self.slice3(x)\n        features3 = x.view(self.batch_size, x.size()[1], -1)\n        gram3 = torch.matmul(features3, torch.transpose(features3, 1, 2)) / features3.size()[1]\n        x = self.slice4(x)\n        features4 = x.view(self.batch_size, x.size()[1], -1)\n        gram4 = torch.matmul(features4, torch.transpose(features4, 1, 2)) / features4.size()[1]\n        x = self.slice5(x)\n        features5 = x.view(self.batch_size, x.size()[1], -1)\n        gram5 = torch.matmul(features5, torch.transpose(features5, 1, 2)) / features5.size()[1]\n        return torch.cat((gram1.view(self.batch_size, -1),\n                         gram2.view(self.batch_size, -1),\n                         gram3.view(self.batch_size, -1),\n                         gram4.view(self.batch_size, -1),\n                         gram5.view(self.batch_size, -1)), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_transform = transforms.Compose([transforms.ToTensor()])\ncocodir = \"coco2017/train2017\"\nfilelist = os.listdir(cocodir)\nclass DataLoader:\n    def __init__(self):\n        return\n    def __getitem__(self, idx):\n        im = Image.open(cocodir + filelist[idx]).convert(\"RGB\")\n        im_shape = np.array(im).shape[:2]\n        proportion = im_shape[1] / im_shape[0]\n        if (min(im_shape) == im_shape[0]):\n            im = im.resize((int(proportion * 600), 600), Image.LANCZOS)\n            left_border = im.size[1] / 2 - 300  \n            im = im.crop((left_border, 0, left_border + 600, 600))\n        else:\n            im = im.resize((600, int(600 / proportion)), Image.LANCZOS)\n            top_border = im.size[0] / 2 - 300\n            im = im.crop((0, top_border, 600, top_border + 600))\n        im = image_transform(im)\n        return im\n    def __len__(self):\n        return len(filelist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (torch.cuda.is_available()):\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_image = Image.open(f\"StylizationImage.jpg\")\nplt.imshow(style_image)\nstyle_image = transforms.ToTensor()(style_image).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrainedContentNet = PretrainedContentNetwork()\npretrainedStyleNet = PretrainedStyleNetwork()\nunet = Unet()\npretrainedStyleNet.to(device)\npretrainedContentNet.to(device)\nunet.to(device)\npretrainedContentNet.eval()\npretrainedStyleNet.eval()\nbatch_size = 2\nloader = torch.utils.data.DataLoader(DataLoader(), batch_size = batch_size)\nstyle_criterion = nn.MSELoss(reduction = 'mean')\ncontent_criterion = nn.MSELoss(reduction = 'mean')\noptimizer = optim.Adam(unet.parameters(), lr = 0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.25, patience = 2000, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_gram = pretrainedStyleNet(style_image.view(1, 3, 600, 600))\nfor i, data in enumerate(loader):\n    optimizer.zero_grad()\n    images = data.to(device)\n    unetOutput = unet(data.to(device))\n    content_source_preds = pretrainedContentNet(images)\n    content_generated_preds = pretrainedContentNet(unetOutput)\n    content_loss = content_criterion(content_source_preds.view(batch_size, -1),\n                                     content_generated_preds.view(batch_size, -1))\n    generated_grams = pretrainedStyleNet(unetOutput)\n    style_loss = style_criterion(generated_grams, style_gram)\n    \n    borderloss = ((unetOutput > 1) * (unetOutput - 1) ** 2) + ((unetOutput < 0) * (-unetOutput) ** 2)\n    loss = 0*style_loss + content_loss + torch.sum(borderloss)\n    loss.backward()\n    optimizer.step()\n    scheduler.step(loss)\n    if (i % 1000 == 0):\n        minimal = torch.min(unetOutput)\n        maximal = torch.max(unetOutput)\n        saveim = (unetOutput[0].view(3, 600, 600) - minimal) / (maximal - minimal)\n        print('\\n', i // 1000 + 1, style_loss, content_loss)\n        print(minimal, maximal)\n        matplotlib.image.imsave(f'source_image{i // 1000}.png', data[0].cpu().detach().numpy().transpose(1, 2, 0))\n        matplotlib.image.imsave(f'generated_image{i // 1000}.png', saveim.cpu().detach().numpy().transpose(1, 2, 0))\n        np.save(f'numpy_image{i // 1000}.npy', saveim.cpu().detach().numpy().transpose(1, 2, 0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}